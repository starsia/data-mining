{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Pattern Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Frequent Pattern Mining (FPM) identifies common co-occurring words or patterns within a dataset. For this project, we applied FPM to analyze a collection of tweets, aiming to uncover recurring themes related to emergencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Before running the FPM algorithm, we cleaned and prepared the dataset:\n",
    "1. **Cleaned the Text**: Removed punctuation, URLs, and stopwords to focus on meaningful words.\n",
    "2. **Handled Missing Data**: Replaced missing values in the `keyword` and `location` columns with empty strings.\n",
    "3. **Created Transactions**: Each tweet was tokenized into words, and the corresponding `keyword` and `location` were added to form transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=34392) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=34392) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend==0.23.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.23.1)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /home/codespace/.local/lib/python3.12/site-packages (from mlxtend==0.23.1) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /home/codespace/.local/lib/python3.12/site-packages (from mlxtend==0.23.1) (2.2.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /home/codespace/.local/lib/python3.12/site-packages (from mlxtend==0.23.1) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/codespace/.local/lib/python3.12/site-packages (from mlxtend==0.23.1) (1.6.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from mlxtend==0.23.1) (3.9.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /home/codespace/.local/lib/python3.12/site-packages (from mlxtend==0.23.1) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend==0.23.1) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend==0.23.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend==0.23.1) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend==0.23.1) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend==0.23.1) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend==0.23.1) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend==0.23.1) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend==0.23.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=0.24.2->mlxtend==0.23.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=0.24.2->mlxtend==0.23.1) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn>=1.0.2->mlxtend==0.23.1) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend==0.23.1) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk\n",
    "! pip install mlxtend==0.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'tweetsv2.csv'  # Adjust path to your file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Clean 'keyword' and 'location' columns\n",
    "data['keyword'] = data['keyword'].fillna('').astype(str)\n",
    "data['location'] = data['location'].fillna('').astype(str)\n",
    "\n",
    "# Apply text cleaning to the 'text' column\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize and prepare transactions\n",
    "data['tokens'] = data['cleaned_text'].apply(lambda x: x.split())\n",
    "data['transactions'] = data.apply(lambda row: row['tokens'] + [row['keyword'], row['location']], axis=1)\n",
    "data['transactions'] = data['transactions'].apply(lambda x: [item for item in x if item and item.lower() != 'nan'])\n",
    "\n",
    "# Remove rows with empty transactions\n",
    "data = data[data['transactions'].apply(len) > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Itemsets\n",
    "Summary of Frequent Itemset Extraction Process\n",
    "Using the Apriori algorithm, we identified frequent itemsets from a dataset of transactions (preprocessed tweet data) with a minimum support threshold of .5%. Frequent itemsets are groups of words (or combinations of words) that occur together in at least .5% of all transactions. The goal of this step is to discover significant co-occurrences of words that form the basis for extracting meaningful patterns and relationships in the data.\n",
    "\n",
    "Why We Used the Apriori Algorithm\n",
    "The Apriori algorithm is a widely used method for frequent pattern mining because:\n",
    "\n",
    "It efficiently identifies itemsets (groups of words) that frequently occur together in a dataset.\n",
    "It systematically prunes infrequent itemsets to reduce computation, ensuring only those meeting the minimum support threshold are retained.\n",
    "It lays the groundwork for generating association rules, which reveal how words relate to each other.\n",
    "The Process in Detail\n",
    "Data Preprocessing:\n",
    "\n",
    "The dataset was cleaned by:\n",
    "Removing URLs and special characters.\n",
    "Converting text to lowercase.\n",
    "Removing English stopwords to focus on meaningful words.\n",
    "Each transaction was constructed from tokenized tweet text, combining relevant columns (text, keyword, and location).\n",
    "One-Hot Encoding:\n",
    "\n",
    "Using the TransactionEncoder, the transactions were transformed into a binary matrix:\n",
    "Rows represented transactions (tweets).\n",
    "Columns represented unique words.\n",
    "Each cell indicated whether a word appeared (True) or did not appear (False) in the transaction.\n",
    "Applying the Apriori Algorithm:\n",
    "\n",
    "A minimum support threshold of .5% was used to identify frequent itemsets.\n",
    "Support measures how often a word or combination of words appears in the dataset relative to the total number of transactions.\n",
    "For example, a support value of 0.01 (1%) means the word or word combination appears in at least 1% of all transactions.\n",
    "The output consisted of itemsets: individual words or combinations of words that co-occur frequently enough to meet the support threshold.\n",
    "Significance of Frequent Itemsets:\n",
    "\n",
    "Frequent itemsets help uncover patterns in the data, such as:\n",
    "Words that often appear together in tweets (e.g., \"fire\" and \"rescue\").\n",
    "Potential associations between keywords and topics.\n",
    "These patterns form the basis for association rules, which provide deeper insights into relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "      support           itemsets\n",
      "0    0.007300        (Australia)\n",
      "1    0.006508            (India)\n",
      "2    0.006069           (London)\n",
      "3    0.007124  (London, England)\n",
      "4    0.006772               (UK)\n",
      "..        ...                ...\n",
      "360  0.018821            (years)\n",
      "361  0.005541              (yes)\n",
      "362  0.006596              (yet)\n",
      "363  0.009411            (youre)\n",
      "364  0.008091    (taal, volcano)\n",
      "\n",
      "[365 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the transactions\n",
    "transactions = data['transactions'].tolist()\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply Apriori algorithm\n",
    "min_support = 0.005  # Adjust threshold as needed\n",
    "frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules\n",
    "Association rules were generated from the frequent itemsets to find relationships between co-occurring words. These rules show patterns where the presence of one word predicts another with high confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Association Rules:\n",
      "  antecedents consequents  confidence       lift\n",
      "0      (taal)   (volcano)    0.741935  59.828415\n",
      "1   (volcano)      (taal)    0.652482  59.828415\n"
     ]
    }
   ],
   "source": [
    "# Generate association rules\n",
    "min_confidence = 0.01  # Adjust confidence threshold as needed\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "# Display rules\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'confidence', 'lift']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Limitations\n",
    "- Frequent Itemsets: Patterns like ['taal', 'volcano'] indicate common co-occurrences in emergency-related tweets.\n",
    "- Sparse Data: Tweets are short, so many patterns consist of single words rather than multi-word itemsets.\n",
    "- Association Rules: While some meaningful rules were generated, many transactions lacked strong co-occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "N-Grams: Use bigrams or trigrams to extract richer context from tweets.\n",
    "Lower Thresholds: Experiment with even lower support and confidence thresholds to uncover less frequent patterns.\n",
    "Alternative Methods: Explore clustering or classification to complement frequent pattern analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
