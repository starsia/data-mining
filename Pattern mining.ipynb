{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Pattern Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Frequent Pattern Mining (FPM) identifies common co-occurring words or patterns within a dataset. For this project, we applied FPM to analyze a collection of tweets, aiming to uncover recurring themes related to emergencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Before running the FPM algorithm, we cleaned and prepared the dataset:\n",
    "1. **Cleaned the Text**: Removed punctuation, URLs, and stopwords to focus on meaningful words.\n",
    "2. **Handled Missing Data**: Replaced missing values in the `keyword` and `location` columns with empty strings.\n",
    "3. **Created Transactions**: Each tweet was tokenized into words, and the corresponding `keyword` and `location` were added to form transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=5658) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=5658) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.23.3)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /home/codespace/.local/lib/python3.12/site-packages (from mlxtend) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /home/codespace/.local/lib/python3.12/site-packages (from mlxtend) (2.2.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /home/codespace/.local/lib/python3.12/site-packages (from mlxtend) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from mlxtend) (1.6.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from mlxtend) (3.9.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /home/codespace/.local/lib/python3.12/site-packages (from mlxtend) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=0.24.2->mlxtend) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=0.24.2->mlxtend) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn>=1.3.1->mlxtend) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk\n",
    "! pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [communal, violence, bhainsa, telangana, stone...\n",
      "1    [telangana, section, imposed, bhainsa, january...\n",
      "2    [arsonist, sets, cars, ablaze, dealership, abl...\n",
      "3    [arsonist, sets, cars, ablaze, dealership, abl...\n",
      "4    [lord, jesus, love, brings, freedom, pardon, f...\n",
      "Name: transactions, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('tweetsv2.csv')\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "data['tokens'] = data['cleaned_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Include keywords and locations in transactions\n",
    "data['keyword'] = data['keyword'].fillna('').astype(str)\n",
    "data['location'] = data['location'].fillna('').astype(str)\n",
    "data['transactions'] = data.apply(lambda row: row['tokens'] + [row['keyword'], row['location']], axis=1)\n",
    "\n",
    "# Verify cleaned transactions\n",
    "print(data['transactions'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Itemsets\n",
    "Using the Apriori algorithm, we extracted frequent itemsets with a minimum support of 1%. Frequent itemsets represent words or combinations of words that appear together in at least 1% of the transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "    support   itemsets\n",
      "0  0.300616         ()\n",
      "1  0.010026      (air)\n",
      "2  0.013369     (also)\n",
      "3  0.044943      (amp)\n",
      "4  0.010026  (another)\n"
     ]
    }
   ],
   "source": [
    "# Transaction Encoding\n",
    "transactions = data['transactions'].tolist()\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply Apriori\n",
    "min_support = 0.01  # Set minimum support\n",
    "frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# Display frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules\n",
    "Association rules were generated from the frequent itemsets to find relationships between co-occurring words. These rules show patterns where the presence of one word predicts another with high confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association Rules:\n",
      "  antecedents consequents  confidence      lift\n",
      "0       (amp)          ()    0.309198  1.028548\n",
      "1        (im)          ()    0.284689  0.947020\n",
      "2      (like)          ()    0.290258  0.965547\n",
      "3       (one)          ()    0.305000  1.014585\n",
      "4    (people)          ()    0.306163  1.018453\n"
     ]
    }
   ],
   "source": [
    "# Generate Association Rules\n",
    "min_confidence = 0.25\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence, num_itemsets=len(frequent_itemsets))\n",
    "# Display rules\n",
    "print(\"Association Rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'confidence', 'lift']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Limitations\n",
    "- Frequent Itemsets: Patterns like ['fire', 'rescue'] indicate common co-occurrences in emergency-related tweets.\n",
    "- Sparse Data: Tweets are short, so many patterns consist of single words rather than multi-word itemsets.\n",
    "- Association Rules: While some meaningful rules were generated, many transactions lacked strong co-occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "N-Grams: Use bigrams or trigrams to extract richer context from tweets.\n",
    "Lower Thresholds: Experiment with even lower support and confidence thresholds to uncover less frequent patterns.\n",
    "Alternative Methods: Explore clustering or classification to complement frequent pattern analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
