{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Pattern Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Frequent Pattern Mining (FPM) identifies common co-occurring words or patterns within a dataset. For this project, we applied FPM to analyze a collection of tweets, aiming to uncover recurring themes related to emergencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Before running the FPM algorithm, we cleaned and prepared the dataset:\n",
    "1. **Cleaned the Text**: Removed punctuation, URLs, and stopwords to focus on meaningful words.\n",
    "2. **Handled Missing Data**: Replaced missing values in the `keyword` and `location` columns with empty strings.\n",
    "3. **Created Transactions**: Each tweet was tokenized into words, and the corresponding `keyword` and `location` were added to form transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk\n",
    "! pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('tweetsv2.csv')\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "data['tokens'] = data['cleaned_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Include keywords and locations in transactions\n",
    "data['keyword'] = data['keyword'].fillna('').astype(str)\n",
    "data['location'] = data['location'].fillna('').astype(str)\n",
    "data['transactions'] = data.apply(lambda row: row['tokens'] + [row['keyword'], row['location']], axis=1)\n",
    "\n",
    "# Verify cleaned transactions\n",
    "print(data['transactions'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Itemsets\n",
    "Using the Apriori algorithm, we extracted frequent itemsets with a minimum support of 1%. Frequent itemsets represent words or combinations of words that appear together in at least 1% of the transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction Encoding\n",
    "transactions = data['transactions'].tolist()\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply Apriori\n",
    "min_support = 0.01  # Set minimum support\n",
    "frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# Display frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules\n",
    "Association rules were generated from the frequent itemsets to find relationships between co-occurring words. These rules show patterns where the presence of one word predicts another with high confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Association Rules\n",
    "min_confidence = 0.5\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "# Display rules\n",
    "print(\"Association Rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'confidence', 'lift']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Limitations\n",
    "- Frequent Itemsets: Patterns like ['fire', 'rescue'] indicate common co-occurrences in emergency-related tweets.\n",
    "- Sparse Data: Tweets are short, so many patterns consist of single words rather than multi-word itemsets.\n",
    "- Association Rules: While some meaningful rules were generated, many transactions lacked strong co-occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "N-Grams: Use bigrams or trigrams to extract richer context from tweets.\n",
    "Lower Thresholds: Experiment with even lower support and confidence thresholds to uncover less frequent patterns.\n",
    "Alternative Methods: Explore clustering or classification to complement frequent pattern analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
