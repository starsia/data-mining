{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "The aim of this section is to apply supervised learning methods to create a classification model to determine if tweets are an actual emergency. With this model, depending on success, could be used to predict a variety of other themes, given the appropriate class labelling. The class label have already been pre-labellel under the column _target_, and shows 1 if the tweet has been classified as an emergency, and 0 if not. The original [file can be found here](/tweets-raw.csv).\n",
    "\n",
    "It is important to preface that this dataset faces the classic class imbalance problem, given that emergency tweets (1) constitutes only 18.5% of records before cleaning. Hence, there will need to be techniques applied to account for class imbalances. For instance, F1-score is more telling than Accuracy measures. We chose to oversample instead of undersample as it would mean disposing of 7k more records of non-emergencies(0), which would mean an even smaller training set after cleaning. \n",
    "\n",
    "Here is the order you will expect as you read the rest of this report:\n",
    "1. [Data pre-processing](#1-data-preprocessing). The tweets are seen as is. For example, besides the actual text, emojis, vulgarities, hashtags are present with varying characters. Location range from actual values such as United States of America to \"hell\" or \"jesus\". \n",
    "2. [Feature engineering](#2-Feature-engineering). The keyword column, which contain the \"emergency\" word in the sentence, will be added to the feature list. The sentences will be tokenised and vectorised using a Term Frequency-Inverse Document Frequency (TF-IDF) approach.\n",
    "3. [Dataset splitting](#3-dataset-splitting). We will need a training set, and a test set. We have decided to employ the holdout method, which uses 2/3 of the data for model training. \n",
    "4. [Model selection](#4-model-selection). We will attempt to use [decision tree induction](#attempted-run-using-decision-tree-classifier-with-significantly-more-execution-time-of-10s), linear regression, and Na√Øve (Complement) Baynesian Classification. We may also further consider ensemble methods, random forest and boosting via AdaBoost. \n",
    "5. [Training phase](#5-training-phase). Models will be applied on keyword and the fragmented sentences as features. \n",
    "6. [Evaluation phase](#6-evaluation-phase). Here, we will apply metrics using the confusion matrix, the Receiver Operating Characteristics Curve and F1-score as previously mentioned. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn\n",
    "! pip install nltk\n",
    "! pip install imblearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "\n",
    "In this phase, we remove punctuations and emojis. Though we are aware that this might affect sentence semantics, especially if we choose to adopt encoder-only transformers, it is relatively easy to roll back. For now, emojis and punctuation will not be considered.\n",
    "\n",
    "We also realised it was important to remove stop words, numbers, and undergo lemmasation (removing of _-ings_). Source: Web Data Mining, Bing Liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('tweetsv2.csv')\n",
    "\n",
    "# Here, we can see that we have a imbalanced data set, with over 3 times the count of non-emergency tweets compared to emergency ones\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure nltk resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('tweetsv2.csv')\n",
    "\n",
    "# Emoji removal source\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)  # Remove all digits\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs starting with http, https, or www\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, emojis, punctuation, and numbers\n",
    "    text = remove_urls(remove_emojis(remove_punctuation(remove_numbers(text))))\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Rejoin the tokens into a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "df['cleantweet'] = df['text'].apply(lambda x: preprocess_text(str(x)))\n",
    "\n",
    "# Display the cleaned data\n",
    "print(df['cleantweet'][0])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleantweet']\n",
    "y = df['target']\n",
    "len(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 33)\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training phase\n",
    "\n",
    "With the dropping of stop words, performance improvements were seen. \n",
    "\n",
    "Original:\n",
    "MNB: 0.846\n",
    "CNB: 0.867\n",
    "SVC: 0.894\n",
    "\n",
    "After stop-words dropped:\n",
    "MNB: 0.857\n",
    "CNB: 0.879\n",
    "SVC: 0.895\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=33)\n",
    "\n",
    "# Step 2: Vectorize training data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)  # Vectorize the training set\n",
    "X_test_vec = vectorizer.transform(X_test)       # Vectorize the test set\n",
    "\n",
    "# Step 3: Oversample on the vectorized training data\n",
    "oversampler = RandomOverSampler(random_state=33)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "# Step 4: Train and evaluate each pipeline\n",
    "# Multinomial Naive Bayes\n",
    "pipeline_MNB = Pipeline([('clf', MultinomialNB())])\n",
    "pipeline_MNB.fit(X_train_resampled, y_train_resampled)\n",
    "predictMNB = pipeline_MNB.predict(X_test_vec)\n",
    "print(f\"MNB: {accuracy_score(y_test, predictMNB):.3f}\")\n",
    "\n",
    "# Complement Naive Bayes\n",
    "pipeline_CNB = Pipeline([('clf', ComplementNB())])\n",
    "pipeline_CNB.fit(X_train_resampled, y_train_resampled)\n",
    "predictCNB = pipeline_CNB.predict(X_test_vec)\n",
    "print(f\"CNB: {accuracy_score(y_test, predictCNB):.3f}\")\n",
    "\n",
    "# Linear SVC\n",
    "pipeline_SVC = Pipeline([('clf', LinearSVC())])\n",
    "pipeline_SVC.fit(X_train_resampled, y_train_resampled)\n",
    "predictSVC = pipeline_SVC.predict(X_test_vec)\n",
    "print(f\"SVC: {accuracy_score(y_test, predictSVC):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempted run using Decision Tree classifier, with significantly longer execution time of >10s\n",
    "Lower accuracy likely due to skewed data distribution. Oversampling recommended due to likelihood of a few leaves being labeled as 'emergency'.\n",
    "\n",
    "Accuracy: 0.809 (after oversampling) from 0.850 (without stop words) from 0.838 (with stop-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt using logistic regression\n",
    "Execution time: 3.3s \n",
    "Accuracy: 0.866 (after oversampling) from 0.875 (without stop words) from 0.850 (with stop-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation phase\n",
    "Given that our dataset is skewed, accuracy as a measure may be less accurate. \n",
    "\n",
    "Let's see our results when the data was not oversampled, and hence, imbalanced.\n",
    "\n",
    "                    precision    recall  f1-score   support\n",
    "\n",
    "                0       0.92      0.96      0.94      3064\n",
    "                1       0.77      0.61      0.68       689\n",
    "\n",
    "         accuracy                           0.90      3753\n",
    "        macro avg       0.84      0.79      0.81      3753\n",
    "     weighted avg       0.89      0.90      0.89      3753\n",
    "\n",
    "\n",
    "- Precision is the number of **correctly classified positive** examples _divided_ by the total number of examples classified as positive. \n",
    "    - In other words, precision = 0.92 for 0 (non-emergency) means 92% of the messages classified as non-emergency are actually non-emergencies. \n",
    "    - Precision = 0.77 means for 1 (emergency) means 77% of the messages classified as emergencies are truly emergencies.\n",
    "\n",
    "- Recall is the number of **correctly classified positive** examples _divided_ by the total number of actual positives in the test set. \n",
    "    - In other words, recall = 0.96 for 0 (non-emergency) means that 96% of the actual non-emergency messages were correctly identified.\n",
    "    - For 1 (emergency), only 61% of the actual emergency messages were identified correctly.\n",
    "\n",
    "- F1-score is the **harmonic mean of precision and recall**, balancing the two.\n",
    "    - An F1 = 0.94 indicates strong performacne for 0 (non-emergency).\n",
    "    - An F1 = 0.68 indicates moderately good performance but room for improvement for 1 (emergency).\n",
    "    - Though it might be tempting to conclude that the lower F1-score for class 1 reflects that the model‚Äôs recall, the support reminds us that the data set is pretty skewed. \n",
    "\n",
    "Now, the data we see below is after oversampling. **Why did precision decrease and recall increase?** Precision decreased because oversampling increases false positives (more non-emergencies classified as emergencies). Recall increased because the model had more examples of emergencies to learn from, reducing false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictSVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, predictSVC)\n",
    "ConfusionMatrixDisplay(cm, display_labels=[0,1]).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample run using Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"Severe weather expected in Lyon, France\"\n",
    "outcome = pipeline_SVC.predict(vectorizer.transform([msg]))\n",
    "print('class label is ' + str(outcome))\n",
    "\n",
    "msg2 = \"Serious problem of scrolling too much instagram in my toilet\"\n",
    "outcome = pipeline_SVC.predict(vectorizer.transform([msg2]))\n",
    "print('class label is ' + str(outcome))\n",
    "\n",
    "msg3 = \"Intense flying cow expected in Lyon\"\n",
    "outcome2 = pipeline_SVC.predict(vectorizer.transform([msg3]))\n",
    "print('class label is ' + str(outcome2))\n",
    "\n",
    "msg4 = \"Climbers lost on hike, uncontactable by friends\"\n",
    "outcome = pipeline_SVC.predict(vectorizer.transform([msg4]))\n",
    "print('class label is ' + str(outcome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
