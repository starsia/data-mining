{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "The aim of this section is to apply supervised learning methods to create a classification model to determine if tweets are an actual emergency. With this model, depending on success, could be used to predict a variety of other themes, given the appropriate class labelling. The class label have already been pre-labellel under the column _target_, and shows 1 if the tweet has been classified as an emergency, and 0 if not. The original [file can be found here](/tweets-raw.csv).\n",
    "\n",
    "It is important to preface that this dataset faces the classic class imbalance problem, given that emergency tweets (1) constitutes only 18.5% of records before cleaning. Hence, there will need to be techniques applied to account for class imbalances. For instance, F1-score is more telling than Accuracy measures. We chose to oversample instead of undersample as it would mean disposing of 7k more records of non-emergencies(0), which would mean an even smaller training set after cleaning. \n",
    "\n",
    "Here is the order you will expect as you read the rest of this report:\n",
    "1. [Data pre-processing](#1-data-preprocessing). The tweets are seen as is. For example, besides the actual text, emojis, vulgarities, hashtags are present with varying characters. Location range from actual values such as United States of America to \"hell\" or \"jesus\". \n",
    "2. [Feature engineering](#2-Feature-engineering). The keyword column, which contain the \"emergency\" word in the sentence, will be added to the feature list. The sentences will be tokenised and vectorised using a Term Frequency-Inverse Document Frequency (TF-IDF) approach.\n",
    "3. Dataset splitting. We will need a training set, and a test set. We have decided to employ the holdout method, which uses 2/3 of the data for model training. \n",
    "4. Model selection. We will attempt to use decision tree induction, linear regression, and Na√Øve (Complement) Baynesian Classification. We may also further consider ensemble methods, random forest and boosting via AdaBoost. \n",
    "5. Training phase. Models will be applied on keyword and the fragmented sentences as features. \n",
    "6. Evaluation phase. Here, we will apply metrics using the confusion matrix, the Receiver Operating Characteristics Curve and F1-score as previously mentioned. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install Keras\n",
    "! pip install tensorflow\n",
    "! pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "\n",
    "In this phase, we remove punctuations and emojis. Though we are aware that this might affect sentence semantics, especially if we choose to adopt encoder-only transformers, it is relatively easy to roll back. For now, emojis and punctuation will not be considered.\n",
    "\n",
    "We also realised it was important to remove stop words, numbers, and undergo lemmasation (removing of _-ings_). Source: Web Data Mining, Bing Liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('tweetsv2.csv')\n",
    "\n",
    "# Here, we can see that we have a imbalanced data set, with over 3 times the count of non-emergency tweets compared to emergency ones\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('tweetsv2.csv')\n",
    "\n",
    "# Emoji removal source taken from: https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "df['cleantweet'] = df['text.1'].apply(lambda x: remove_emojis(remove_punctuation(str(x))))\n",
    "\n",
    "print(df['cleantweet'][0])\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = df['cleantweet']\n",
    "classlabel = df['target']\n",
    "len(feature1)\n",
    "classlabel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "feature1_train, feature1_test, classlabel_train, classlabel_test = train_test_split(feature1, classlabel, test_size = 0.33, random_state = 33)\n",
    "len(feature1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training phase\n",
    "\n",
    "With the inclusion of stop words, performance improvements were seen. \n",
    "\n",
    "Original:\n",
    "\n",
    "MNB: 0.846\n",
    "CNB: 0.867\n",
    "SVC: 0.894\n",
    "\n",
    "After stop-words dropped:\n",
    "MNB: 0.856\n",
    "CNB: 0.863\n",
    "SVC: 0.892\n",
    "\n",
    "After n-gram range added:\n",
    "MNB: 0.847\n",
    "CNB: 0.889\n",
    "SVC: 0.893\n",
    "\n",
    "stop_words = 'english'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for the different classification functions\n",
    "pipeline_MNB = Pipeline([('tfidf', TfidfVectorizer(stop_words = 'english', ngram_range=(1,3))), ('clf', MultinomialNB())])\n",
    "pipeline_CNB = Pipeline([('tfidf', TfidfVectorizer(stop_words = 'english', ngram_range=(1,3))), ('clf', ComplementNB())])\n",
    "pipeline_SVC = Pipeline([('tfidf', TfidfVectorizer(stop_words = 'english', ngram_range=(1,3))), ('clf', LinearSVC())])\n",
    "\n",
    "pipeline_MNB.fit(feature1_train, classlabel_train)\n",
    "predictMNB = pipeline_MNB.predict(feature1_test)\n",
    "print(f\"MNB: {accuracy_score(classlabel_test, predictMNB):.3f}\")\n",
    "\n",
    "pipeline_CNB.fit(feature1_train, classlabel_train)\n",
    "predictCNB = pipeline_CNB.predict(feature1_test)\n",
    "print(f\"CNB: {accuracy_score(classlabel_test, predictCNB):.3f}\")\n",
    "\n",
    "pipeline_SVC.fit(feature1_train, classlabel_train)\n",
    "predictSVC = pipeline_SVC.predict(feature1_test)\n",
    "print(f\"SVC: {accuracy_score(classlabel_test, predictSVC):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(classlabel_test, predictSVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"Severe weather expected in Lyon\"\n",
    "outcome = pipeline_SVC.predict([msg])\n",
    "print('class label is ' + str(outcome))\n",
    "\n",
    "msg2 = \"Intense flying cow expected in Lyon\"\n",
    "outcome2 = pipeline_SVC.predict([msg2])\n",
    "print('class label is ' + str(outcome2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
